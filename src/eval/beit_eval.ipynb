{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeitFeatureExtractor, BeitForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, IterableDataset, get_worker_info\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import sys\n",
    "import logging\n",
    "sys.path.append('/scratch/bf996/vlhub/src')\n",
    "from training.imagenet_zeroshot_data import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-base-patch16-224')\n",
    "model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\n",
    "# inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "# logits = outputs.logits\n",
    "# # model predicts one of the 1000 ImageNet classes\n",
    "# predicted_class_idx = logits.argmax(-1).item()\n",
    "# print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEpoch:\n",
    "    def __init__(self, epoch: int = 0):\n",
    "        self.shared_epoch = Value('i', epoch)\n",
    "\n",
    "    def set_value(self, epoch):\n",
    "        self.shared_epoch.value = epoch\n",
    "\n",
    "    def get_value(self):\n",
    "        return self.shared_epoch.value\n",
    "\n",
    "@dataclass\n",
    "class DataInfo:\n",
    "    dataloader: DataLoader\n",
    "    sampler: DistributedSampler = None\n",
    "    shared_epoch: SharedEpoch = None\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        if self.shared_epoch is not None:\n",
    "            self.shared_epoch.set_value(epoch)\n",
    "        if self.sampler is not None and isinstance(self.sampler, DistributedSampler):\n",
    "            self.sampler.set_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imagenet(args, preprocess_fns, split):\n",
    "    assert split in [\"train\", \"val\", \"v2\", \"r\", \"a\", \"s\"], \"Not a recognized ImageNet split, {}\".format(split)\n",
    "    is_train = (split == \"train\")\n",
    "    preprocess_train = preprocess_val = preprocess_fns\n",
    "\n",
    "    if split == \"v2\":\n",
    "        from imagenetv2_pytorch import ImageNetV2Dataset\n",
    "        dataset = ImageNetV2Dataset(location=args[\"imagenet_v2\"], transform=preprocess_val)\n",
    "    elif is_train:\n",
    "        data_path = args.imagenet_train\n",
    "        preprocess_fn = preprocess_train\n",
    "        dataset = datasets.ImageFolder(data_path, transform=preprocess_train)\n",
    "    else:\n",
    "        if split == \"val\":\n",
    "            data_path = args['imagenet_val']\n",
    "        if split == \"r\":\n",
    "            data_path = args['imagenet_r']\n",
    "        if split == \"a\":\n",
    "            data_path = args['imagenet_a']\n",
    "        if split == \"s\":\n",
    "            data_path = args['imagenet_s']\n",
    "        preprocess_fn = preprocess_val\n",
    "        assert data_path, \"No data path found\"\n",
    "\n",
    "        dataset = datasets.ImageFolder(data_path, transform=preprocess_val)\n",
    "    if is_train:\n",
    "        idxs = np.zeros(len(dataset.targets))\n",
    "        target_array = np.array(dataset.targets)\n",
    "        k = 50\n",
    "        for c in range(1000):\n",
    "            m = target_array == c\n",
    "            n = len(idxs[m])\n",
    "            arr = np.zeros(n)\n",
    "            arr[:k] = 1\n",
    "            np.random.shuffle(arr)\n",
    "            idxs[m] = arr\n",
    "\n",
    "        idxs = idxs.astype('int')\n",
    "        sampler = SubsetRandomSampler(np.where(idxs)[0])\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args['batch_size'],\n",
    "        num_workers=args['workers'],\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    return DataInfo(dataloader=dataloader, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--imagenet-a \"/imagenet-a\" --imagenet-r \"/imagenet-r\" --imagenet-val \"/imagenet/val/\" --imagenet-v2 \"/scratch/bf996/datasets\" --imagenet-s \"/imagenet-sketch\"\n",
    "\n",
    "args = {\"imagenet_v2\" : \"/scratch/bf996/datasets\", \"imagenet_r\" : \"/imagenet-r\", \"imagenet_val\" : \"/imagenet/val/\", \"imagenet_a\" : \"/imagenet-a\", \"imagenet_s\" : \"/imagenet-sketch\", \"batch_size\" : 32, \"workers\" : 8, 'device' : 'cuda:0'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize, Compose, RandomResizedCrop, InterpolationMode, ToTensor, Resize, \\\n",
    "    CenterCrop\n",
    "\n",
    "def _convert_to_rgb(image):\n",
    "    return image.convert('RGB')\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "transform_l = [Resize(image_size, interpolation=InterpolationMode.BICUBIC), CenterCrop(image_size), _convert_to_rgb, ToTensor(), Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))]\n",
    "\n",
    "transform_c = Compose(transform_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data[\"imagenet-val\"] = get_imagenet(args, transform_c, \"val\")\n",
    "data[\"imagenet-v2\"] = get_imagenet(args, transform_c, \"v2\")\n",
    "data[\"imagenet-s\"] = get_imagenet(args, transform_c, \"s\")\n",
    "data[\"imagenet-r\"] = get_imagenet(args, transform_c, \"r\")\n",
    "data[\"imagenet-a\"] = get_imagenet(args, transform_c, \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, classifier, dataloader, args, idx=None, split=None, caption_subset=\"\"):\n",
    "    with torch.no_grad():\n",
    "        top1, top5, n = 0., 0., 0.\n",
    "        for images, target in tqdm(dataloader, unit_scale=args['batch_size']):\n",
    "            if caption_subset != \"\":\n",
    "                if split == \"r\":\n",
    "                    ir_idx = get_ir_idx().tolist()\n",
    "                    match_idx = sum(target==ir_idx.index(i) for i in idx).bool().nonzero(as_tuple=True)[0]\n",
    "                elif split == \"a\":\n",
    "                    ia_idx = get_ia_idx().tolist()\n",
    "                    #keep only the samples which are in passed-in class subset, using correct imagenet-a indices \n",
    "                    match_idx = sum(target==ia_idx.index(i) for i in idx).bool().nonzero(as_tuple=True)[0]\n",
    "                else:\n",
    "                    match_idx = sum(target==i for i in idx).bool().nonzero(as_tuple=True)[0]\n",
    "                #shave down target and images size so we skip irrelevant samples\n",
    "                target = target[match_idx]\n",
    "                images = images[match_idx]\n",
    "            if images.size(0) == 0:\n",
    "                continue\n",
    "            model = model.to(\"cuda:0\")\n",
    "            images = images.to(\"cuda:0\")\n",
    "            target = target.to(\"cuda:0\")\n",
    "            logits = model(images).logits\n",
    "            #zero out logits which are not being evaluated (in VL this is handled by changing the size of the classification problem)\n",
    "            if caption_subset != \"\":\n",
    "                icap_idx = get_icap_idx(caption_subset)\n",
    "                not_icap_idx = [i for i in range(1000) if i not in icap_idx]\n",
    "                logits[:, not_icap_idx] = float(\"-inf\")\n",
    "            if split == 'r':\n",
    "                ir_idx = get_ir_idx()\n",
    "                not_ir_idx = [i for i in range(1000) if i not in ir_idx]\n",
    "                logits[:, not_ir_idx] = float(\"-inf\")\n",
    "            if split == 'a':\n",
    "                ia_idx = get_ia_idx()\n",
    "                not_ia_idx = [i for i in range(1000) if i not in ia_idx]\n",
    "                logits[:, not_ia_idx] = float(\"-inf\")\n",
    "\n",
    "            acc1, acc5 = accuracy(logits, target, topk=(1, min(5, len(icap_idx))))\n",
    "            n += images.size(0)\n",
    "            top1 += acc1\n",
    "            top5 += acc5\n",
    "            #print(\"top1\", top1, \"n\", n)\n",
    "\n",
    "    top1 = (top1 / n)\n",
    "    top5 = (top5 / n)\n",
    "    #TODO: debug integer labels for extended metrics\n",
    "    return top1, top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Starting zero-shot imagenet.')\n",
    "caption_subset = \"in100\"\n",
    "if caption_subset != \"\":\n",
    "    logging.info(\"Using caption subset {}\".format(caption_subset))\n",
    "    get_icap_idx(caption_subset)\n",
    "    get_common_ir_idx()\n",
    "    get_common_ir_idx_zeroindexed()\n",
    "    get_common_ia_idx()\n",
    "    get_common_ia_idx_zeroindexed()\n",
    "    get_common_obj_idx()\n",
    "    get_common_obj_idx_zeroindexed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50016/50016 [00:55<00:00, 904.77it/s] \n",
      "100%|██████████| 10016/10016 [00:18<00:00, 546.92it/s]\n",
      "100%|██████████| 50912/50912 [01:43<00:00, 493.77it/s]\n",
      "100%|██████████| 30016/30016 [00:48<00:00, 620.71it/s] \n",
      "100%|██████████| 7520/7520 [00:10<00:00, 696.31it/s] \n"
     ]
    }
   ],
   "source": [
    "classifier = None\n",
    "imagenets = []\n",
    "results = {}\n",
    "if 'imagenet-val' in data:            \n",
    "    top1, top5 = run(model, classifier, data['imagenet-val'].dataloader, args, get_icap_idx(caption_subset) if caption_subset != \"\" else None, caption_subset=caption_subset)\n",
    "    results['imagenet-zeroshot-val-top1'] = top1\n",
    "    imagenets.append(top1)\n",
    "    results['imagenet-zeroshot-val-top5'] = top5\n",
    "    print('Finished zero-shot val. Top1 was {}, top5 was {}'.format(top1, top5))\n",
    "if 'imagenet-v2' in data:\n",
    "    top1, top5 = run(model, classifier, data['imagenet-v2'].dataloader, args, get_icap_idx(caption_subset) if caption_subset != \"\" else None, caption_subset=caption_subset)\n",
    "    results['imagenetv2-zeroshot-val-top1'] = top1\n",
    "    imagenets.append(top1)\n",
    "    results['imagenetv2-zeroshot-val-top5'] = top5\n",
    "    print('Finished zero-shot v2. Top1 was {}, top5 was {}'.format(top1, top5))\n",
    "if 'imagenet-s' in data:\n",
    "    top1, top5 = run(model, classifier, data['imagenet-s'].dataloader, args, get_icap_idx(caption_subset) if caption_subset != \"\" else None, caption_subset=caption_subset)\n",
    "    results['imagenets-zeroshot-val-top1'] = top1\n",
    "    imagenets.append(top1)\n",
    "    results['imagenets-zeroshot-val-top5'] = top5\n",
    "    print('Finished zero-shot sketch. Top1 was {}, top5 was {}'.format(top1, top5))\n",
    "if 'imagenet-r' in data:\n",
    "    top1, top5 = run(model, classifier, data['imagenet-r'].dataloader, args, get_common_ir_idx() if caption_subset != \"\" else get_ir_idx(), \"r\", caption_subset=caption_subset)\n",
    "    results['imagenetr-zeroshot-val-top1'] = top1\n",
    "    imagenets.append(top1)\n",
    "    results['imagenetr-zeroshot-val-top5'] = top5\n",
    "    print('Finished zero-shot imagenet-r. Top1 was {}, top5 was {}'.format(top1, top5))\n",
    "if 'imagenet-a' in data:\n",
    "    top1, top5 = run(model, classifier, data['imagenet-a'].dataloader, args, get_common_ia_idx() if caption_subset != \"\" else get_ia_idx(), \"a\", caption_subset=caption_subset)\n",
    "    results['imageneta-zeroshot-val-top1'] = top1\n",
    "    imagenets.append(top1)\n",
    "    results['imageneta-zeroshot-val-top5'] = top5\n",
    "    print('Finished zero-shot imagenet-a. Top1 was {}, top5 was {}'.format(top1, top5))  \n",
    "if results.get('imagenet-zeroshot-val-top1'):\n",
    "    logging.info(\"computing effective robustness on imagenet\")\n",
    "    logging.info(\"len imagenets {}\".format(len(imagenets)))\n",
    "    try:\n",
    "        imagenet_shifts = []\n",
    "        for shift in ['imagenetr-zeroshot-val-top1', 'imageneta-zeroshot-val-top1', 'imagenets-zeroshot-val-top1', 'imagenetv2-zeroshot-val-top1']:\n",
    "            if results.get(shift):\n",
    "                imagenet_shifts.append(results[shift])\n",
    "        if len(imagenet_shifts) > 0:\n",
    "            results['imagenet-average-robustness'] = np.average(imagenet_shifts)\n",
    "            results['imagenet-effective-robustness'] = np.divide(np.average(imagenet_shifts), results['imagenet-zeroshot-val-top1'])\n",
    "            print(\"Average robustness over {} ImageNet shifts: {}\".format(len(imagenet_shifts), results['imagenet-average-robustness']))\n",
    "    except Exception as e:\n",
    "        logging.info(\"error calculating effective robustness: \")\n",
    "        logging.info(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imagenet-zeroshot-val-top1': 0.4228, 'imagenet-zeroshot-val-top5': 0.6152, 'imagenetv2-zeroshot-val-top1': 0.352, 'imagenetv2-zeroshot-val-top5': 0.526, 'imagenets-zeroshot-val-top1': 0.02242770017706079, 'imagenets-zeroshot-val-top5': 0.046429274050757426, 'imagenetr-zeroshot-val-top1': 0.0001585791309863622, 'imagenetr-zeroshot-val-top5': 0.0009514747859181732, 'imageneta-zeroshot-val-top1': 0.0006246096189881324, 'imageneta-zeroshot-val-top5': 0.0037476577139287947, 'imagenet-average-robustness': 0.09380272223175881, 'imagenet-effective-robustness': 0.22186074321608043}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEIT Eval from timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN100_TRUE_IDX = [386, 928, 931, 704, 907, 291, 454, 76, 952, 788, 245, 937, 924, 8, 983, 816, 920, 379, 204, 396, 929, 619, 815, 88, 84, 217, 118, 935, 987, 642, 950, 951, 954, 557, 18, 967, 945, 6, 440, 348, 22, 571, 23, 963, 104, 958, 579, 312, 534, 620, 115, 298, 284, 552, 373, 997, 182, 422, 308, 839, 13, 489, 805, 832, 85, 695, 2, 863, 310, 565, 886, 455, 988, 347, 580, 425, 99, 424, 105, 107, 343, 658, 721, 443, 421, 679, 19, 825, 130, 309, 849, 879, 496, 971, 922, 985, 286, 625, 637, 943]\n",
    "ir_idx = [1, 2, 4, 6, 8, 9, 11, 13, 22, 23, 26, 29, 31, 39, 47, 63, 71, 76, 79, 84, 90, 94, 96, 97, 99, 100, 105, 107, 113, 122, \n",
    "125, 130, 132, 144, 145, 147, 148, 150, 151, 155, 160, 161, 162, 163, 171, 172, 178, 187, 195, 199, 203, 207, 208, 219, \n",
    "231, 232, 234, 235, 242, 245, 247, 250, 251, 254, 259, 260, 263, 265, 267, 269, 276, 277, 281, 288, 289, 291, 292, 293, \n",
    "296, 299, 301, 308, 309, 310, 311, 314, 315, 319, 323, 327, 330, 334, 335, 337, 338, 340, 341, 344, 347, 353, 355, 361, \n",
    "362, 365, 366, 367, 368, 372, 388, 390, 393, 397, 401, 407, 413, 414, 425, 428, 430, 435, 437, 441, 447, 448, 457, 462, \n",
    "463, 469, 470, 471, 472, 476, 483, 487, 515, 546, 555, 558, 570, 579, 583, 587, 593, 594, 596, 609, 613, 617, 621, 629, \n",
    "637, 657, 658, 701, 717, 724, 763, 768, 774, 776, 779, 780, 787, 805, 812, 815, 820, 824, 833, 847, 852, 866, 875, 883, \n",
    "889, 895, 907, 928, 931, 932, 933, 934, 936, 937, 943, 945, 947, 948, 949, 951, 953, 954, 957, 963, 965, 967, 980, 981, \n",
    "983, 988]\n",
    "IN100_DOGS_IDX = [151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250]\n",
    "ia_idx = [6, 11, 13, 15, 17, 22, 23, 27, 30, 37, 39, 42, 47, 50, 57, 70, 71, 76, 79, 89, 90, 94, 96, 97, 99, 105, 107, 108, 110, \n",
    "113, 124, 125, 130, 132, 143, 144, 150, 151, 207, 234, 235, 254, 277, 283, 287, 291, 295, 298, 301, 306, 307, 308, 309, \n",
    "310, 311, 313, 314, 315, 317, 319, 323, 324, 326, 327, 330, 334, 335, 336, 347, 361, 363, 372, 378, 386, 397, 400, 401, \n",
    "402, 404, 407, 411, 416, 417, 420, 425, 428, 430, 437, 438, 445, 456, 457, 461, 462, 470, 472, 483, 486, 488, 492, 496, \n",
    "514, 516, 528, 530, 539, 542, 543, 549, 552, 557, 561, 562, 569, 572, 573, 575, 579, 589, 606, 607, 609, 614, 626, 627, \n",
    "640, 641, 642, 643, 658, 668, 677, 682, 684, 687, 701, 704, 719, 736, 746, 749, 752, 758, 763, 765, 768, 773, 774, 776, \n",
    "779, 780, 786, 792, 797, 802, 803, 804, 813, 815, 820, 823, 831, 833, 835, 839, 845, 847, 850, 859, 862, 870, 879, 880, \n",
    "888, 890, 897, 900, 907, 913, 924, 932, 933, 934, 937, 943, 945, 947, 951, 954, 956, 957, 959, 971, 972, 980, 981, 984, \n",
    "986, 987, 988]\n",
    "common_ia_idx = [n for n in ia_idx if n in IN100_TRUE_IDX]\n",
    "common_ir_idx = [n for n in ir_idx if n in IN100_TRUE_IDX]\n",
    "common_dir_idx = [n for n in ir_idx if n in IN100_DOGS_IDX]\n",
    "common_dia_idx = [n for n in ia_idx if n in IN100_DOGS_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.659"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/scratch/bf996/caption-paper-ICLR/timm_results_tfeffs-da.csv\")\n",
    "df = df[df['targets'].isin(common_dia_idx)]\n",
    "round(len(df[df['index'] == df['targets']]) / len(df), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itorch",
   "language": "python",
   "name": "itorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
